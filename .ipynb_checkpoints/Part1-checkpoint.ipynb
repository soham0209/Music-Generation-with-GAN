{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Tutorial Part 1 (Intro to pytorch, linear regression and neural networks)\n",
    "Alan Ritter, Ohio State University\n",
    "\n",
    "Portions adapted from Pytorch tutorial by Justin Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's generate a random (regression) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7208, 0.6609, 0.6895, 0.1177, 0.4334, 0.1400, 0.8343, 0.9763, 0.2755,\n",
      "         0.5469],\n",
      "        [0.7759, 0.3053, 0.6611, 0.6532, 0.3164, 0.2139, 0.7463, 0.5057, 0.8818,\n",
      "         0.4592],\n",
      "        [0.1251, 0.9306, 0.7558, 0.8154, 0.4443, 0.0165, 0.7029, 0.4335, 0.8956,\n",
      "         0.8348],\n",
      "        [0.8063, 0.3091, 0.3728, 0.4536, 0.2751, 0.9637, 0.6239, 0.8424, 0.3730,\n",
      "         0.1390],\n",
      "        [0.4389, 0.6490, 0.1259, 0.0055, 0.0930, 0.2968, 0.2896, 0.4596, 0.6572,\n",
      "         0.0835],\n",
      "        [0.8341, 0.7809, 0.6508, 0.0366, 0.6756, 0.2515, 0.1661, 0.0022, 0.2199,\n",
      "         0.1409],\n",
      "        [0.0953, 0.6206, 0.3546, 0.6872, 0.7086, 0.3697, 0.0729, 0.8270, 0.3981,\n",
      "         0.1874],\n",
      "        [0.3903, 0.5942, 0.7852, 0.2264, 0.5011, 0.3918, 0.5273, 0.0711, 0.9862,\n",
      "         0.2251],\n",
      "        [0.9153, 0.1766, 0.2200, 0.4600, 0.1372, 0.2017, 0.2361, 0.6755, 0.3285,\n",
      "         0.4383],\n",
      "        [0.8806, 0.5171, 0.0113, 0.3275, 0.9627, 0.0665, 0.7663, 0.2118, 0.9866,\n",
      "         0.1044],\n",
      "        [0.9745, 0.5488, 0.4203, 0.7256, 0.0377, 0.5963, 0.4251, 0.4477, 0.4682,\n",
      "         0.1656],\n",
      "        [0.2966, 0.2129, 0.9823, 0.5408, 0.6563, 0.0792, 0.3192, 0.5446, 0.0544,\n",
      "         0.9220],\n",
      "        [0.5029, 0.7082, 0.5511, 0.4202, 0.5752, 0.8137, 0.1479, 0.7796, 0.3725,\n",
      "         0.7135],\n",
      "        [0.6515, 0.1866, 0.6296, 0.0953, 0.1908, 0.9470, 0.0089, 0.9943, 0.7568,\n",
      "         0.4783],\n",
      "        [0.5898, 0.0232, 0.4659, 0.5891, 0.5501, 0.8452, 0.1275, 0.0819, 0.1477,\n",
      "         0.0600],\n",
      "        [0.2122, 0.0874, 0.1217, 0.0482, 0.6201, 0.4533, 0.5335, 0.6543, 0.3904,\n",
      "         0.8198],\n",
      "        [0.8209, 0.9825, 0.5438, 0.6828, 0.8738, 0.5671, 0.9674, 0.5048, 0.7675,\n",
      "         0.0991],\n",
      "        [0.3960, 0.0881, 0.9108, 0.7452, 0.6494, 0.6704, 0.6019, 0.5405, 0.0921,\n",
      "         0.3871],\n",
      "        [0.2667, 0.2360, 0.5727, 0.1151, 0.1208, 0.0371, 0.0338, 0.0007, 0.7602,\n",
      "         0.1816],\n",
      "        [0.1846, 0.2516, 0.1314, 0.1423, 0.1559, 0.2450, 0.9760, 0.5735, 0.0660,\n",
      "         0.1859],\n",
      "        [0.7835, 0.8473, 0.1340, 0.6162, 0.0582, 0.2178, 0.2761, 0.8290, 0.3359,\n",
      "         0.6908],\n",
      "        [0.1891, 0.5566, 0.8922, 0.0293, 0.5889, 0.7744, 0.1460, 0.0422, 0.7217,\n",
      "         0.9161],\n",
      "        [0.0019, 0.6496, 0.5499, 0.0434, 0.8731, 0.3806, 0.3529, 0.6432, 0.0888,\n",
      "         0.8675],\n",
      "        [0.7251, 0.1990, 0.7832, 0.2310, 0.2674, 0.5717, 0.8450, 0.0129, 0.0730,\n",
      "         0.3324],\n",
      "        [0.4080, 0.7508, 0.3128, 0.1598, 0.4860, 0.0665, 0.8154, 0.3441, 0.6475,\n",
      "         0.1925],\n",
      "        [0.7044, 0.9233, 0.9944, 0.4696, 0.5437, 0.4453, 0.0389, 0.6175, 0.1076,\n",
      "         0.6083],\n",
      "        [0.9193, 0.4636, 0.6114, 0.5020, 0.1066, 0.2724, 0.3500, 0.6673, 0.8138,\n",
      "         0.0074],\n",
      "        [0.1874, 0.4219, 0.1786, 0.9280, 0.5184, 0.2909, 0.4227, 0.4141, 0.7956,\n",
      "         0.2824],\n",
      "        [0.0923, 0.5194, 0.0768, 0.8745, 0.5552, 0.7876, 0.3865, 0.8040, 0.2950,\n",
      "         0.4679],\n",
      "        [0.1835, 0.6454, 0.6323, 0.4978, 0.4692, 0.8682, 0.2813, 0.3958, 0.3453,\n",
      "         0.7393],\n",
      "        [0.0271, 0.5343, 0.8145, 0.6478, 0.0034, 0.2598, 0.5008, 0.3156, 0.1392,\n",
      "         0.7607],\n",
      "        [0.8435, 0.6846, 0.2601, 0.6783, 0.5632, 0.4054, 0.4175, 0.6856, 0.6670,\n",
      "         0.6094],\n",
      "        [0.8294, 0.3847, 0.4204, 0.6604, 0.3532, 0.3414, 0.1509, 0.8619, 0.2997,\n",
      "         0.1970],\n",
      "        [0.3986, 0.0002, 0.1274, 0.6803, 0.8734, 0.6154, 0.4721, 0.7671, 0.4997,\n",
      "         0.3465],\n",
      "        [0.2143, 0.4515, 0.5884, 0.5849, 0.2363, 0.4330, 0.8777, 0.1689, 0.7962,\n",
      "         0.8688],\n",
      "        [0.7858, 0.4719, 0.6471, 0.7483, 0.9044, 0.8771, 0.8844, 0.8607, 0.6610,\n",
      "         0.1198],\n",
      "        [0.7960, 0.9211, 0.7406, 0.8545, 0.6225, 0.2964, 0.4122, 0.7867, 0.1044,\n",
      "         0.8607],\n",
      "        [0.6509, 0.1794, 0.5584, 0.1820, 0.6488, 0.1987, 0.4716, 0.4120, 0.0812,\n",
      "         0.0746],\n",
      "        [0.8182, 0.3008, 0.7161, 0.5241, 0.4961, 0.6603, 0.7693, 0.8785, 0.8478,\n",
      "         0.1595],\n",
      "        [0.5722, 0.4051, 0.9379, 0.5503, 0.1058, 0.2728, 0.6226, 0.9340, 0.3824,\n",
      "         0.4926],\n",
      "        [0.5837, 0.1983, 0.1166, 0.6680, 0.5933, 0.4821, 0.1754, 0.4445, 0.0852,\n",
      "         0.3259],\n",
      "        [0.6783, 0.8949, 0.8022, 0.3439, 0.7714, 0.6737, 0.3680, 0.1572, 0.2136,\n",
      "         0.4738],\n",
      "        [0.6430, 0.5447, 0.3411, 0.9450, 0.7073, 0.2475, 0.6358, 0.6557, 0.5085,\n",
      "         0.4201],\n",
      "        [0.7203, 0.2144, 0.2154, 0.8494, 0.0468, 0.9853, 0.6433, 0.0135, 0.4020,\n",
      "         0.9971],\n",
      "        [0.1157, 0.3572, 0.1224, 0.5075, 0.3672, 0.2003, 0.2564, 0.4958, 0.0662,\n",
      "         0.7910],\n",
      "        [0.4220, 0.5792, 0.8353, 0.1820, 0.8511, 0.1657, 0.1047, 0.0611, 0.8729,\n",
      "         0.0543],\n",
      "        [0.7130, 0.2777, 0.6545, 0.9342, 0.4452, 0.4175, 0.9465, 0.5883, 0.8299,\n",
      "         0.6277],\n",
      "        [0.1772, 0.8282, 0.6440, 0.2533, 0.1748, 0.6476, 0.6671, 0.7486, 0.7038,\n",
      "         0.8129],\n",
      "        [0.6506, 0.9776, 0.1674, 0.2556, 0.4115, 0.0177, 0.6650, 0.2451, 0.7779,\n",
      "         0.6671],\n",
      "        [0.6859, 0.1463, 0.9948, 0.1387, 0.1461, 0.3423, 0.6053, 0.2330, 0.9926,\n",
      "         0.0832]])\n",
      "tensor([-1.0159, -0.7456,  0.1795, -0.9047,  1.1524])\n"
     ]
    }
   ],
   "source": [
    "in_dim = 10\n",
    "out_dim = 5\n",
    "\n",
    "#Randomly generate a dataset.\n",
    "X     = th.rand(50,in_dim)\n",
    "Y     = th.randn(50,out_dim)\n",
    "\n",
    "print(X[0,:])\n",
    "print(Y[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initalize Parameters\n",
    "W = th.randn(in_dim,out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1339.5227)\n",
      "loss=tensor(238.7469)\n",
      "loss=tensor(207.9514)\n",
      "loss=tensor(201.3121)\n",
      "loss=tensor(199.3423)\n",
      "loss=tensor(198.5982)\n",
      "loss=tensor(198.2677)\n",
      "loss=tensor(198.1061)\n",
      "loss=tensor(198.0230)\n",
      "loss=tensor(197.9788)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(1000):\n",
    "    #Predict outputs\n",
    "    output = X.mm(W)\n",
    "    \n",
    "    #Squared error\n",
    "    loss = (output - Y).pow(2).sum()\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        #print((output.shape, X.shape, W.shape))\n",
    "        print(\"loss=%s\" % loss)\n",
    "    \n",
    "    #Compute Gradient\n",
    "    error  = 2.0 * (output - Y)\n",
    "    grad_W = X.t().mm(error)\n",
    "    \n",
    "    #Update the parameters\n",
    "    W  -= learning_rate * grad_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Weights for a Neural Network with One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 100\n",
    "\n",
    "#Initialize Weights (initialized from Gaussian distribution with 0 mean and 1 variance)\n",
    "W_in  = th.randn(in_dim,hidden_dim)\n",
    "W_out = th.randn(hidden_dim,out_dim)\n",
    "\n",
    "#print(W_in)\n",
    "#print(W_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network with manually computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(4808.4966)\n",
      "loss=tensor(147.8679)\n",
      "loss=tensor(92.2713)\n",
      "loss=tensor(58.1469)\n",
      "loss=tensor(38.0554)\n",
      "loss=tensor(25.1213)\n",
      "loss=tensor(16.7602)\n",
      "loss=tensor(11.4410)\n",
      "loss=tensor(7.9654)\n",
      "loss=tensor(5.6048)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(10000):\n",
    "    #Forward Pass\n",
    "    h         = X.mm(W_in)\n",
    "    h_sigmoid = th.sigmoid(h)\n",
    "    output    = h_sigmoid.mm(W_out)\n",
    "    \n",
    "    #Squared error\n",
    "    loss = (output - Y).pow(2).sum()\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print(\"loss=%s\" % loss)\n",
    "    \n",
    "    #Backward Pass (compute gradients)\n",
    "    grad_output     = 2.0 * (output - Y)\n",
    "    grad_W_out      = h_sigmoid.t().mm(grad_output)\n",
    "    grad_h_sigmoid  = grad_output.mm(W_out.t())\n",
    "    grad_h          = grad_h_sigmoid * h_sigmoid * (1-h_sigmoid)\n",
    "    grad_W_in       = X.t().mm(grad_h)\n",
    "    \n",
    "    #Update the parameters\n",
    "    W_in  -= learning_rate * grad_W_in\n",
    "    W_out -= learning_rate * grad_W_out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now, the same thing, but with gradients computed automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=9548.947265625\n",
      "loss=151.18746948242188\n",
      "loss=95.04609680175781\n",
      "loss=59.54445266723633\n",
      "loss=38.978336334228516\n",
      "loss=25.69999885559082\n",
      "loss=17.442073822021484\n",
      "loss=12.360021591186523\n",
      "loss=9.058777809143066\n",
      "loss=6.748473644256592\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "data_type = torch.FloatTensor # CPU\n",
    "# data_type = torch.cuda.FloatTensor # GPU\n",
    "\n",
    "hidden_dim = 100\n",
    "\n",
    "#Initialize Weights (initialized from Gaussian distribution with 0 mean and 1 variance)\n",
    "W_in  = Variable(th.randn(in_dim,hidden_dim).type(data_type), requires_grad=True)\n",
    "W_out = Variable(th.randn(hidden_dim,out_dim).type(data_type), requires_grad=True)\n",
    "\n",
    "X_var = Variable(X.type(data_type), requires_grad=False)\n",
    "Y_var = Variable(Y.type(data_type), requires_grad=False)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for i in range(10000):\n",
    "    #Forward Pass\n",
    "    output = X_var.mm(W_in).sigmoid().mm(W_out)\n",
    "    \n",
    "    #Squared error\n",
    "    loss = (output - Y_var).pow(2).sum()\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print(\"loss=%s\" % loss.item())\n",
    "    \n",
    "    #The backward pass is very simple:\n",
    "    loss.backward()\n",
    "    \n",
    "    ################################################################\n",
    "    # Now, we don't have to do this by hand!\n",
    "    ################################################################\n",
    "    #grad_output     = 2.0 * (output - Y)\n",
    "    #grad_W_out      = h_sigmoid.t().mm(grad_output)\n",
    "    #grad_h_sigmoid  = grad_output.mm(W_out.t())\n",
    "    #grad_h          = grad_h_sigmoid * h_sigmoid * (1-h_sigmoid)\n",
    "    #grad_W_in       = X.t().mm(grad_h)\n",
    "    ################################################################\n",
    "    \n",
    "    #Update the parameters\n",
    "    W_in.data  -= learning_rate * W_in.grad.data\n",
    "    W_out.data -= learning_rate * W_out.grad.data\n",
    "    \n",
    "    W_in.grad.data.zero_()\n",
    "    W_out.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do it again,  this time using the nn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.116289734840393\n",
      "loss=0.9731495976448059\n",
      "loss=0.9483171105384827\n",
      "loss=0.9248232841491699\n",
      "loss=0.9023768305778503\n",
      "loss=0.881300151348114\n",
      "loss=0.8621872067451477\n",
      "loss=0.8455711603164673\n",
      "loss=0.8316991925239563\n",
      "loss=0.820482611656189\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "data_type = torch.FloatTensor # CPU\n",
    "# data_type = torch.cuda.FloatTensor # GPU\n",
    "\n",
    "hidden_dim = 100\n",
    "\n",
    "X_var = Variable(X.type(data_type), requires_grad=False)\n",
    "Y_var = Variable(Y.type(data_type), requires_grad=False)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_dim, hidden_dim),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(hidden_dim,out_dim)\n",
    ")\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    #Forward Pass\n",
    "    output = model(X_var)\n",
    "    \n",
    "    #Squared error\n",
    "    loss = mse_loss(output, Y_var)\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print(\"loss=%s\" % loss.item())\n",
    "    \n",
    "    #The backward pass is very simple:\n",
    "    loss.backward()\n",
    "    \n",
    "    #Update the parameters\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data\n",
    "    \n",
    "    model.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now using Adagrad (torch.optim)\n",
    "References on Adagrad:\n",
    "- http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n",
    "- https://cs.stanford.edu/~ppasupat/a9online/1107.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7208, 0.6609, 0.6895, 0.1177, 0.4334, 0.1400, 0.8343, 0.9763, 0.2755,\n",
      "        0.5469])\n",
      "tensor([0.7208, 0.6609, 0.6895, 0.1177, 0.4334, 0.1400, 0.8343, 0.9763, 0.2755,\n",
      "        0.5469])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "data_type = torch.FloatTensor # CPU\n",
    "# data_type = torch.cuda.FloatTensor # GPU\n",
    "\n",
    "hidden_dim = 100\n",
    "\n",
    "X_var = Variable(X.type(data_type), requires_grad=False)\n",
    "Y_var = Variable(Y.type(data_type), requires_grad=False)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_dim, hidden_dim),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(hidden_dim,out_dim)\n",
    ")\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "#Adagrad\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "print(X_var[0])\n",
    "\n",
    "for i in range(10000):\n",
    "    #Forward Pass\n",
    "    output = model(X_var)\n",
    "    \n",
    "    #Squared error\n",
    "    loss = mse_loss(output, Y_var)\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print(output)\n",
    "    \n",
    "    #The backward pass is very simple:\n",
    "    loss.backward()\n",
    "    \n",
    "    #Update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.zero_grad()\n",
    "print(X_var[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
